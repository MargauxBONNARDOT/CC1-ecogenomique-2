---
title: "R Notebook"
output: github_document
---

```{bash}
sudo apt-get update -y
sudo apt-get install -y libglpk-dev 
sudo apt-get install -y liblzma-dev libbz2-dev
```

```{r}
# package de r-studio
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("BiocStyle")
BiocManager::install("Rhtslib")
#biocmanager est un site de depot, les pakages depedent d'autres packages et biomarqueur regle tous pour nous
# on verif que tous a ete installé et on instale d'autre trucs
```

```{r}
library("knitr")
library("BiocStyle")
.cran_packages <- c("ggplot2", "gridExtra", "devtools")
install.packages(.cran_packages) 
.bioc_packages <- c("dada2", "phyloseq", "DECIPHER", "phangorn")
BiocManager::install(.bioc_packages)
# Load packages into session, and print package version
sapply(c(.cran_packages, .bioc_packages), require, character.only = TRUE)
```

```{bash}
cd~
wget https://mothur.s3.us-east-2.amazonaws.com/wiki/miseqsopdata.zip
unzip miseqsopdata.zip
```

````{r}
set.seed(100)
miseq_path <- "./MiSeq_SOP" 
list.files(miseq_path)
````

````{r}
set.seed(100)
miseq_path <- "/home/rstudio/MiSeq_SOP"
list.files(miseq_path)
````

````{r}
# Si on quite R(pas R studio) il faut run ca pour qu'il ait bien les packages
.cran_packages <- c("ggplot2", "gridExtra", "devtools")
.bioc_packages <- c("dada2", "phyloseq", "DECIPHER", "phangorn")
sapply(c(.cran_packages, .bioc_packages), require, character.only = TRUE)
````


````{r}
# Sort to unsure foward and reverse reads are in the same order

fnFs <- sort(list.files(miseq_path, pattern="_R1_001.fastq"))
fnRs <- sort(list.files(miseq_path, pattern="_R2_001.fastq"))
#F c'est pour foward et R pour reverse

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sampleNames <- sapply(strsplit(fnFs, "_"), `[`, 1)
# apply
# on garde que ce qui est séparé par underscore

# Specify the full path to the fnFs and fnRs
# on change ce qu'il y a ds objets
# contruire un chemin vers fichier à partir de composant, independant de la plateforme
fnFs <- file.path(miseq_path, fnFs)
fnRs <- file.path(miseq_path, fnRs)
````

````{r}
fnFs[1:3] #affiches les 3 premier elements de fnFs
fnRs[1:3]
````


Most Illumina sequencing data shows a trend of decreasing average quality towards the end of sequencing reads.
The first two forward reads:
````{r}
# va lire les 2 premiers elements en Foward
plotQualityProfile(fnFs[1:2])
# score de qualité
#heat map de la freq des score de qualité pour chacune des positions
# donc en noir: pour tous les read, le score de qualité associé a chacune des bases le plus frequent c'est 38
#bon score de qualité vers 38/39
# Q score va de 0 à 50, à 30, 1/1000 chance (en fait y'3 zero =30) que cette base la est pas la bonne base

#la ligne rouge:
#illumina: read de 250
#--> distribution des longueur de read
#plus une seq est longue, plus on a d'erreur


# pour les reverses
plotQualityProfile(fnRs[1:2])
#perte de qualité à partir d'une longueur de 125
# a fin de seq on a score de qualité de 25


# Nous on veut garder que les sequences de bonnes qualité
# illumina faut 2x250bp
# V4 fait 300pb
# on va essayer aligner foward et reverse pour reformer les v4
#mais si on enleve toute mauvase qualité, ca enleve le chevauchement!!! donc faut conserver alignement (au moins 1à, mais vauut mieux alignement de 20)


#Here, the forward reads maintain high quality throughout, while the quality of the reverse reads drops significantly #at about position 160. Therefore, we choose to truncate the forward reads at position 245, and the reverse reads at #position 160. 
#We also choose to trim the first 10 nucleotides of each read based on empirical observations across many Illumina #datasets that these base positions are particularly likely to contain pathological errors.
#la fonction plotQualityProfile s'en charge

````

````{r}

#We define the filenames for the filtered fastq.gz files:

  #on creer un path filtered
  # on le met dans filtered/ subdirectory
filt_path <- file.path(miseq_path, "filtered") # "filtered" est le nouv endroit ou ya. nos objet
if(!file_test("-d", filt_path)) 
  dir.create(filt_path)
  #le -d correspond à un element du file path
  # si file_test n'est pas -d et dans le filtered path, alors dir.create provide a low-level
  #interface to the computer's file system.

filtFs <- file.path(filt_path, paste0(sampleNames, "_F_filt.fasta.gz")) #filtered path pour les fowards, on cherche le filtered path et on trie par le nom du sample
filtRs <- file.path(filt_path, paste0(sampleNames, "_R_filt.fastq.gz"))
````

````{r}
# combine trimming parameters with standard filtering parameters

# Filter the forward and reverse reads:
# on utilise filter and tim
# cette fonction prend une fichier fastq suivant certain critère def et contient les reads trimmed
# en imput on met les files de reverse et foward
# dans ce cas le filtering is performed on the forward and reverse reads independently, and both reads must pass for the read pair to be output.


out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) 
# out filte et compresse tous ca dans le fichier

# On Windows set multithread=FALSE
head(out)
````


````{r}
# Infer sequence variants

  #  high-resolution DADA2 method
  # --> infer amplicon sequence variants (ASVs) 
  # name the resulting "derep-class" objects by their sample name.


# Dereplication
  # la fonction de replicating amplicon sequences from fastq or compressed fastq files, while also controlling peak memory   requirement to support large files.
  # combines all identical sequencing reads into into “unique sequences” # with a corresponding “abundance”
  derepFs <- derepFastq(filtFs, verbose=TRUE)
  derepRs <- derepFastq(filtRs, verbose=TRUE)

  # Name the derep-class objects by the sample names
  names(derepFs) <- sampleNames #en fait il recoivent les nom des ech
  names(derepRs) <- sampleNames


  errF <- learnErrors(filtFs, multithread=TRUE) # error F ou R 
  errR <- learnErrors(filtRs, multithread=TRUE)
  # 33514080 total bases in 139642 reads from 20 samples will be used for #learning the error rates.
  #22342720 total bases in 139642 reads from 20 samples will be used for #learning the error rates.
  # --> ce sont les profil d'erreur

  plotErrors(errF)
  plotErrors(errR)
  # verify that the error rates
  # --> show the frequencies of each type of transition as a function of #the quality.
  #pour chacune des mutation posible: que A remplace A à une proba tres haute, ou C par C, T par T ......

  # difference entre OTU (a cause erreur de seq, on a pleins de variant)
  # si on retire variant, on a plus OTU, donc 2 unité ecologique proches sont considéré comme pareille

  # pooling improves the detection of rare variants
  dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
  dadaRs <- dada(derepRs, err=errR, multithread=TRUE)

# Inspecting the dada-class object returned by dada:
  
  dadaFs[[1]] #liste de liste
  # 128 sequence variants were inferred from 1979 



#Construct sequence table and remove chimeras

  # sequence table --> sample by sequence feature table valued by the 
  # number of times each sequence was observed in each sample.
  mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)
  # merge each denoised pair of forward and reverse reads, rejecting any pairs which do not 
  # sufficiently overlap or contain too many
  
  # cnostruction table seq qui resume combien de fois est obs chacun des asv
  seqtabAll <- makeSequenceTable(mergers[!grepl("Mock", names(mergers))])
  table(nchar(getSequences(seqtabAll)))
  # 251 252 253 254 255 
  #  1  85 186   5   2
  # Notably, chimeras have not yet been removed
  #nombre de fois dans lequel apparaissent echantillions

  seqtabNoC <- removeBimeraDenovo(seqtabAll)
  # chimerasmake up about 22% of the inferred sequence variants
  # but they are rare (4%)
  seqtabNoC
````

````{r}
# Assign taxonomy
  # --> dada2 package, va comparer les otu qu'ona fomer et va les identifier ?
  # --> cluster à 97% d'identifier


  # on doit telecharger :   silva_nr99_v138.1_train_set.fa
  # faut le mettre sur la VM

  #fastaRef <- "/home/rdp_train_set_16.fa.gz"
  #taxTab <- assignTaxonomy(seqtabNoC, refFasta = fastaRef, multithread=TRUE) 
  #unname(head(taxTab))

  #fastaRef <- "/home/ecogenomique2/ecogenomiqueM1MFAprojet/silva_nr99_v138.1_train_set.fa"
  fastaRef <- "/home/rstudio/silva_nr99_v138.1_train_set.fa"
  taxTab <- assignTaxonomy(seqtabNoC, refFasta = fastaRef, multithread=TRUE) 
  unname(head(taxTab))
  taxTab
  
````

````{r}
#Construct phylogenetic tree

  #calculation of phylogeny-aware distances between microbial communities
  seqs <- getSequences(seqtabNoC) # fonction de dada 2 qui recup seq ds seqtaNoC auquel ont a enelevé les chimere
  # montre niv occurence de chaque ech
  # table d'obs

  names(seqs) <- seqs # This propagates to the tip labels of the tree
  alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA,verbose=FALSE)

  # The phangorn R package is then used to construct a phylogenetic tree
  phangAlign <- phyDat(as(alignment, "matrix"), type="DNA")
  dm <- dist.ml(phangAlign)
  treeNJ <- NJ(dm) # Note, tip order != sequence order
  fit = pml(treeNJ, data=phangAlign)
  fitGTR <- update(fit, k=4, inv=0.2)
  fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
        rearrangement = "stochastic", control = pml.control(trace = 0))
     detach("package:phangorn", unload=TRUE)
  
  fitGTR
    #Rate matrix:
   # a c g t
  #a 0 1 1 1
  #c 1 0 1 1
  #g 1 1 0 1
  #t 1 1 1 0

  #Base frequencies:  
  #0.25 0.25 0.25 0.25
  
  plot(fitGTR)

````


````{r}
# Combine data into a phyloseq object

  samdf <- read.csv("https://raw.githubusercontent.com/spholmes/F1000_workflow/master/data/MIMARKS_Data_combined.csv",header=TRUE)
  samdf$SampleID <- paste0(gsub("00", "", samdf$host_subject_id), "D", samdf$age-21)
  samdf <- samdf[!duplicated(samdf$SampleID),] # Remove dupicate entries for reverse reads
  rownames(seqtabAll) <- gsub("124", "125", rownames(seqtabAll)) # Fix discrepancy
  all(rownames(seqtabAll) %in% samdf$SampleID) # TRUE


  rownames(samdf) <- samdf$SampleID
  keep.cols <- c("collection_date", "biome", "target_gene", "target_subfragment","host_common_name", "host_subject_id", "age", "sex", "body_product", "tot_mass", "diet", "family_relationship", "genotype", "SampleID") 
  samdf <- samdf[rownames(seqtabAll), keep.cols]

  #The full suite of data for this study – the sample-by-sequence feature table, the sample metadata, 
  #the sequence taxonomies, and the phylogenetic tree – can now be combined into a single object.
  ps <- phyloseq(otu_table(seqtabNoC, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxTab),phy_tree(fitGTR$tree))
  ps <- prune_samples(sample_names(ps) != "Mock", ps) # Remove mock sample
  ps
  
    ## phyloseq-class experiment-level object
    ## otu_table()   OTU Table:         [ 215 taxa and 19 samples ]
    ## sample_data() Sample Data:       [ 19 samples by 14 sample variables ]
    ## tax_table()   Taxonomy Table:    [ 215 taxa by 6 taxonomic ranks ]
     ## phy_tree()    Phylogenetic Tree: [ 215 tips and 213 internal nodes ]
````


````{r}
# Using phyloseq
  # phyloseq is an R package 
  # table metadonnées des OTU

# Loading the data
  # pas beosin de le faire (recuperer objet phylosec sur lien)
  # "https://raw.githubusercontent.com/spholmes/F1000_workflow/master/data/ps.rds"


# Filtering
  #Taxonomic Filtering
    # on peut enlever seq en se basant sur asignation taxonomique
    # seq non indentifiée (quand phylum est null ou uncharacterize)
    rank_names(ps)
    table(tax_table(ps)[, "Phylum"], exclude = NULL)
    ps <- subset_taxa(ps, !is.na(Phylum) & !Phylum %in% c("", "uncharacterized"))

    prevdf = apply(X = otu_table(ps),
                  MARGIN = ifelse(taxa_are_rows(ps), yes = 1, no = 2),
                   FUN = function(x){sum(x > 0)})
    # Add taxonomy and total read counts to this data.frame
    prevdf = data.frame(Prevalence = prevdf,
                        TotalAbundance = taxa_sums(ps),
                        tax_table(ps))
    
    # Compute the total and average prevalences of the features in each phylum.
    plyr::ddply(prevdf, "Phylum", function(df1){cbind(mean(df1$Prevalence),sum(df1$Prevalence))})

    
    # Define phyla to filter
      filterPhyla = c("Fusobacteria", "Deinococcus-Thermus")
      # Filter entries with unidentified Phylum.
      ps1 = subset_taxa(ps, !Phylum %in% filterPhyla)
      ps1
      
      
````


```{r}
.cran_packages <- c( "shiny","miniUI", "caret", "pls", "e1071", "ggplot2", "randomForest", "dplyr", "ggrepel", "nlme", "devtools",
                  "reshape2", "PMA", "structSSI", "ade4",
                  "ggnetwork", "intergraph", "scales")
.github_packages <- c("jfukuyama/phyloseqGraphTest")
.bioc_packages <- c("genefilter", "impute")
```

```{r}
.inst <- .cran_packages %in% installed.packages()
if (any(!.inst)){
  install.packages(.cran_packages[!.inst],repos = "http://cran.rstudio.com/")
}
```

```{r}
.inst <- .github_packages %in% installed.packages()
if (any(!.inst)){
  devtools::install_github(.github_packages[!.inst])
}
```

```{r}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)){BiocManager::install(.bioc_packages[!.inst])
}
```

premiere note:
Faire jusque ordination
;md est interpreté par github comme page web
#titre
##sous titre
###sous sous titre
mettre dans 2 script separé instalation des package(library/sapply) et l'autre 
car il faut pas refaire instalation des packages

2 eme note:
exercice: influence saisons et profondeur sur microbiote (voir slack)

3eme note:
donnée de l'article qu'on a presenter
https://www.ebi.ac.uk/ena/browser/view/SRX6374530?show=reads
hhh


