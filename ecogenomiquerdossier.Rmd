---
title: "R Notebook"
output: github_document
---

```{bash}
sudo apt-get update -y
sudo apt-get install -y libglpk-dev 
sudo apt-get install -y liblzma-dev libbz2-dev
```

```{r}
# package de r-studio
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("BiocStyle")
BiocManager::install("Rhtslib")
#biocmanager est un site de depot, les pakages depedent d'autres packages et biomarqueur regle tous pour nous
# on verif que tous a ete installé et on instale d'autre trucs
```

```{r}
library("knitr")
library("BiocStyle")
.cran_packages <- c("ggplot2", "gridExtra", "devtools")
install.packages(.cran_packages) 
.bioc_packages <- c("dada2", "phyloseq", "DECIPHER", "phangorn")
BiocManager::install(.bioc_packages)
# Load packages into session, and print package version
sapply(c(.cran_packages, .bioc_packages), require, character.only = TRUE)
```

```{bash}
cd~
wget https://mothur.s3.us-east-2.amazonaws.com/wiki/miseqsopdata.zip
unzip miseqsopdata.zip
```

````{r}
set.seed(100)
miseq_path <- "./MiSeq_SOP" 
list.files(miseq_path)
````

````{r}
set.seed(100)
miseq_path <- "/home/rstudio/MiSeq_SOP"
list.files(miseq_path)
````

````{r}
# Si on quite R(pas R studio) il faut run ca pour qu'il ait bien les packages
.cran_packages <- c("ggplot2", "gridExtra", "devtools")
.bioc_packages <- c("dada2", "phyloseq", "DECIPHER", "phangorn")
sapply(c(.cran_packages, .bioc_packages), require, character.only = TRUE)
````


````{r}
# Sort to unsure foward and reverse reads are in the same order

fnFs <- sort(list.files(miseq_path, pattern="_R1_001.fastq"))
fnRs <- sort(list.files(miseq_path, pattern="_R2_001.fastq"))
#F c'est pour foward et R pour reverse

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sampleNames <- sapply(strsplit(fnFs, "_"), `[`, 1)
# apply
# on garde que ce qui est séparé par underscore

# Specify the full path to the fnFs and fnRs
# on change ce qu'il y a ds objets
# contruire un chemin vers fichier à partir de composant, independant de la plateforme
fnFs <- file.path(miseq_path, fnFs)
fnRs <- file.path(miseq_path, fnRs)
````

````{r}
fnFs[1:3] #affiches les 3 premier elements de fnFs
fnRs[1:3]
````


Most Illumina sequencing data shows a trend of decreasing average quality towards the end of sequencing reads.
The first two forward reads:
````{r}
# va lire les 2 premiers elements en Foward
plotQualityProfile(fnFs[1:2])
# score de qualité
#heat map de la freq des score de qualité pour chacune des positions
# donc en noir: pour tous les read, le score de qualité associé a chacune des bases le plus frequent c'est 38
#bon score de qualité vers 38/39
# Q score va de 0 à 50, à 30, 1/1000 chance (en fait y'3 zero =30) que cette base la est pas la bonne base

#la ligne rouge:
#illumina: read de 250
#--> distribution des longueur de read
#plus une seq est longue, plus on a d'erreur


# pour les reverses
plotQualityProfile(fnRs[1:2])
#perte de qualité à partir d'une longueur de 125
# a fin de seq on a score de qualité de 25


# Nous on veut garder que les sequences de bonnes qualité
# illumina faut 2x250bp
# V4 fait 300pb
# on va essayer aligner foward et reverse pour reformer les v4
#mais si on enleve toute mauvase qualité, ca enleve le chevauchement!!! donc faut conserver alignement (au moins 1à, mais vauut mieux alignement de 20)


#Here, the forward reads maintain high quality throughout, while the quality of the reverse reads drops significantly #at about position 160. Therefore, we choose to truncate the forward reads at position 245, and the reverse reads at #position 160. 
#We also choose to trim the first 10 nucleotides of each read based on empirical observations across many Illumina #datasets that these base positions are particularly likely to contain pathological errors.
#la fonction plotQualityProfile s'en charge

````

````{r}

#We define the filenames for the filtered fastq.gz files:

  #on creer un path filtered
  # on le met dans filtered/ subdirectory
filt_path <- file.path(miseq_path, "filtered") # "filtered" est le nouv endroit ou ya. nos objet
if(!file_test("-d", filt_path)) 
  dir.create(filt_path)
  #le -d correspond à un element du file path
  # si file_test n'est pas -d et dans le filtered path, alors dir.create provide a low-level
  #interface to the computer's file system.

filtFs <- file.path(filt_path, paste0(sampleNames, "_F_filt.fasta.gz")) #filtered path pour les fowards, on cherche le filtered path et on trie par le nom du sample
filtRs <- file.path(filt_path, paste0(sampleNames, "_R_filt.fastq.gz"))
````

````{r}
# combine trimming parameters with standard filtering parameters

# Filter the forward and reverse reads:
# on utilise filter and tim
# cette fonction prend une fichier fastq suivant certain critère def et contient les reads trimmed
# en imput on met les files de reverse et foward
# dans ce cas le filtering is performed on the forward and reverse reads independently, and both reads must pass for the read pair to be output.


out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160), maxN=0, maxEE=c(2,2),truncQ=2, rm.phix=TRUE,compress=TRUE, multithread=TRUE)
# out filte et compresse tous ca dans le fichier

# On Windows set multithread=FALSE
head(out)
````


````{r}
# Infer sequence variants

  #  high-resolution DADA2 method
  # --> infer amplicon sequence variants (ASVs) 
  # name the resulting "derep-class" objects by their sample name.


# Dereplication
  # la fonction de replicating amplicon sequences from fastq or compressed fastq files, while also controlling peak memory   requirement to support large files.
  # combines all identical sequencing reads into into “unique sequences” # with a corresponding “abundance”
  derepFs <- derepFastq(filtFs, verbose=TRUE)
  derepRs <- derepFastq(filtRs, verbose=TRUE)

  # Name the derep-class objects by the sample names
  names(derepFs) <- sampleNames #en fait il recoivent les nom des ech
  names(derepRs) <- sampleNames


  errF <- learnErrors(filtFs, multithread=TRUE) # error F ou R 
  errR <- learnErrors(filtRs, multithread=TRUE)
  # 33514080 total bases in 139642 reads from 20 samples will be used for #learning the error rates.
  #22342720 total bases in 139642 reads from 20 samples will be used for #learning the error rates.
  # --> ce sont les profil d'erreur

  plotErrors(errF)
  plotErrors(errR)
  # verify that the error rates
  # --> show the frequencies of each type of transition as a function of #the quality.
  #pour chacune des mutation posible: que A remplace A à une proba tres haute, ou C par C, T par T ......

  # difference entre OTU (a cause erreur de seq, on a pleins de variant)
  # si on retire variant, on a plus OTU, donc 2 unité ecologique proches sont considéré comme pareille

  # pooling improves the detection of rare variants
  dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
  dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
````

````{r}  
  
# Inspecting the dada-class object returned by dada:
  
  dadaFs[[1]] #liste de liste
  # 128 sequence variants were inferred from 1979 
````

````{r}
#Construct sequence table and remove chimeras

  # sequence table --> sample by sequence feature table valued by the 
  # number of times each sequence was observed in each sample.
  mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)
  # merge each denoised pair of forward and reverse reads, rejecting any pairs which do not 
  # sufficiently overlap or contain too many
  
  # cnostruction table seq qui resume combien de fois est obs chacun des asv
  seqtabAll <- makeSequenceTable(mergers[!grepl("Mock", names(mergers))])
  table(nchar(getSequences(seqtabAll)))
  # 251 252 253 254 255 
  #  1  85 186   5   2
  # Notably, chimeras have not yet been removed
  #nombre de fois dans lequel apparaissent echantillions

  seqtabNoC <- removeBimeraDenovo(seqtabAll)
  # chimerasmake up about 22% of the inferred sequence variants
  # but they are rare (4%)
  seqtabNoC
````

````{r}
# Assign taxonomy
  # --> dada2 package, va comparer les otu qu'ona fomer et va les identifier ?
  # --> cluster à 97% d'identifier


  # on doit telecharger :   silva_nr99_v138.1_train_set.fa
  # faut le mettre sur la VM

  #fastaRef <- "/home/rdp_train_set_16.fa.gz"
  #taxTab <- assignTaxonomy(seqtabNoC, refFasta = fastaRef, multithread=TRUE) 
  #unname(head(taxTab))

  #fastaRef <- "/home/ecogenomique2/ecogenomiqueM1MFAprojet/silva_nr99_v138.1_train_set.fa"
  fastaRef <- "/home/rstudio/silva_nr99_v138.1_train_set.fa"
  taxTab <- assignTaxonomy(seqtabNoC, refFasta = fastaRef, multithread=TRUE) 
  unname(head(taxTab))
  taxTab
  
````

````{r}
#Construct phylogenetic tree

  #calculation of phylogeny-aware distances between microbial communities
  seqs <- getSequences(seqtabNoC) # fonction de dada 2 qui recup seq ds seqtaNoC auquel ont a enelevé les chimere
  # montre niv occurence de chaque ech
  # table d'obs

  names(seqs) <- seqs # This propagates to the tip labels of the tree
  alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA,verbose=FALSE)

  # The phangorn R package is then used to construct a phylogenetic tree
  phangAlign <- phyDat(as(alignment, "matrix"), type="DNA")
  dm <- dist.ml(phangAlign)
  treeNJ <- NJ(dm) # Note, tip order != sequence order
  fit = pml(treeNJ, data=phangAlign)
  fitGTR <- update(fit, k=4, inv=0.2)
  fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
        rearrangement = "stochastic", control = pml.control(trace = 0))
     detach("package:phangorn", unload=TRUE)
  
  fitGTR
    #Rate matrix:
   # a c g t
  #a 0 1 1 1
  #c 1 0 1 1
  #g 1 1 0 1
  #t 1 1 1 0

  #Base frequencies:  
  #0.25 0.25 0.25 0.25
  
  plot(fitGTR)

````


````{r}
# Combine data into a phyloseq object

  samdf <- read.csv("https://raw.githubusercontent.com/spholmes/F1000_workflow/master/data/MIMARKS_Data_combined.csv",header=TRUE)
  samdf$SampleID <- paste0(gsub("00", "", samdf$host_subject_id), "D", samdf$age-21)
  samdf <- samdf[!duplicated(samdf$SampleID),] # Remove dupicate entries for reverse reads
  rownames(seqtabAll) <- gsub("124", "125", rownames(seqtabAll)) # Fix discrepancy
  all(rownames(seqtabAll) %in% samdf$SampleID) # TRUE


  rownames(samdf) <- samdf$SampleID
  keep.cols <- c("collection_date", "biome", "target_gene", "target_subfragment","host_common_name", "host_subject_id", "age", "sex", "body_product", "tot_mass", "diet", "family_relationship", "genotype", "SampleID") 
  samdf <- samdf[rownames(seqtabAll), keep.cols]

  #The full suite of data for this study – the sample-by-sequence feature table, the sample metadata, 
  #the sequence taxonomies, and the phylogenetic tree – can now be combined into a single object.
  ps <- phyloseq(otu_table(seqtabNoC, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxTab),phy_tree(fitGTR$tree))
  ps <- prune_samples(sample_names(ps) != "Mock", ps) # Remove mock sample
  ps
  
    ## phyloseq-class experiment-level object
    ## otu_table()   OTU Table:         [ 215 taxa and 19 samples ]
    ## sample_data() Sample Data:       [ 19 samples by 14 sample variables ]
    ## tax_table()   Taxonomy Table:    [ 215 taxa by 6 taxonomic ranks ]
     ## phy_tree()    Phylogenetic Tree: [ 215 tips and 213 internal nodes ]
````


````{r}
# Using phyloseq
  # phyloseq is an R package 
  # table metadonnées des OTU

# Loading the data
  # pas beosin de le faire (recuperer objet phylosec sur lien)
  # "https://raw.githubusercontent.com/spholmes/F1000_workflow/master/data/ps.rds"


# Filtering
  #Taxonomic Filtering
    # on peut enlever seq en se basant sur asignation taxonomique
    # seq non indentifiée (quand phylum est null ou uncharacterize)
    rank_names(ps)
    table(tax_table(ps)[, "Phylum"], exclude = NULL)
    ps <- subset_taxa(ps, !is.na(Phylum) & !Phylum %in% c("", "uncharacterized"))

    prevdf = apply(X = otu_table(ps),
                  MARGIN = ifelse(taxa_are_rows(ps), yes = 1, no = 2),
                   FUN = function(x){sum(x > 0)})
    # Add taxonomy and total read counts to this data.frame
    prevdf = data.frame(Prevalence = prevdf,
                        TotalAbundance = taxa_sums(ps),
                        tax_table(ps))
    
    # Compute the total and average prevalences of the features in each phylum.
    plyr::ddply(prevdf, "Phylum", function(df1){cbind(mean(df1$Prevalence),sum(df1$Prevalence))})

    
    # Define phyla to filter
      filterPhyla = c("Fusobacteria", "Deinococcus-Thermus")
      # Filter entries with unidentified Phylum.
      ps1 = subset_taxa(ps, !Phylum %in% filterPhyla)
      ps1
      
      
````




```{r}
.cran_packages <- c( "shiny","miniUI", "caret", "pls", "e1071", "ggplot2", "randomForest", "dplyr", "ggrepel", "nlme", "devtools",
                  "reshape2", "PMA", "structSSI", "ade4",
                  "ggnetwork", "intergraph", "scales")
.github_packages <- c("jfukuyama/phyloseqGraphTest")
.bioc_packages <- c("genefilter", "impute")
```

```{r}
.inst <- .cran_packages %in% installed.packages()
if (any(!.inst)){
  install.packages(.cran_packages[!.inst],repos = "http://cran.rstudio.com/")
}
```

```{r}
.inst <- .github_packages %in% installed.packages()
if (any(!.inst)){
  devtools::install_github(.github_packages[!.inst])
}
```

```{r}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)){BiocManager::install(.bioc_packages[!.inst])
}
```



```{r}
# Prevalence Filtering

  # Subset to the remaining phyla
# Include a guess for parameter
prevdf1 = subset(prevdf, Phylum %in% get_taxa_unique(ps1, "Phylum"))

ggplot(prevdf1, aes(TotalAbundance, Prevalence / nsamples(ps),color=Phylum)) + geom_hline(yintercept = 0.05, alpha = 0.5, linetype = 2) +  geom_point(size = 2, alpha = 0.7) + scale_x_log10() +  xlab("Total Abundance") + ylab("Prevalence [Frac. Samples]") + facet_wrap(~Phylum) + theme(legend.position="none")



  # Define prevalence threshold as 5% of total samples
  prevalenceThreshold = 0.05 * nsamples(ps)
  prevalenceThreshold
  # Execute prevalence filter, using `prune_taxa()` function
  keepTaxa = rownames(prevdf1)[(prevdf1$Prevalence >= prevalenceThreshold)]
  ps2 = prune_taxa(keepTaxa, ps)
  
  
# Agglomerate taxa
  # How many genera would be present after filtering?
  length(get_taxa_unique(ps2, taxonomic.rank = "Genus"))
  ps3 = tax_glom(ps2, "Genus", NArm = TRUE)
  
  h1 = 0.4
  ps4 = tip_glom(ps2, h = h1)
  multiPlotTitleTextSize = 15
  p2tree = plot_tree(ps2, method = "treeonly",ladderize = "left",title = "Before Agglomeration") + theme(plot.title = element_text(size = multiPlotTitleTextSize)) 
  p3tree = plot_tree(ps3, method = "treeonly", ladderize = "left", title = "By Genus") + theme(plot.title = element_text(size = multiPlotTitleTextSize))
  p4tree = plot_tree(ps4, method = "treeonly",ladderize = "left", title = "By Height") + theme(plot.title = element_text(size = multiPlotTitleTextSize))
  
  grid.arrange(nrow = 1, p2tree, p3tree, p4tree)
``` 




```{r}  
# Abundance value transformation
  

  # Arbitrary subset, based on Phylum, for plotting

  plot_abundance = function(physeq,title = "", Facet = "Order", Color = "Phylum"){
      p1f = subset_taxa(physeq, Phylum %in% c("Firmicutes"))
      mphyseq = psmelt(p1f)
      mphyseq <- subset(mphyseq, Abundance > 0)
     ggplot(data = mphyseq, mapping = aes_string(x = "sex",y = "Abundance",color = Color, fill = Color)) +geom_violin(fill = NA) + geom_point(size = 1, alpha = 0.3, position = position_jitter(width = 0.3)) + facet_wrap(facets = Facet) + scale_y_log10()+ theme(legend.position="none")
  }

```
 
 
```{r}
  
  # Transform to relative abundance. Save as new object.
  ps3ra = transform_sample_counts(ps3, function(x){x / sum(x)})

  plotBefore = plot_abundance(ps3,"")
  plotAfter = plot_abundance(ps3ra,"")
  # Combine each plot into one graphic.
  grid.arrange(nrow = 2,  plotBefore, plotAfter)
```


```{r}  
  
# Subset by taxonomy
  psOrd = subset_taxa(ps3ra, Order == "Lactobacillales")
  plot_abundance(psOrd, Facet = "Genus", Color = NULL)
  

  
```

```{r}
#Preprocessing
  qplot(sample_data(ps)$age, geom = "histogram",binwidth=20) + xlab("age")
  qplot(log10(rowSums(otu_table(ps))),binwidth=0.2) +
  xlab("Logged counts-per-sample")
  
  
  sample_data(ps)$age_binned <- cut(sample_data(ps)$age,breaks = c(0, 100, 200, 400))
levels(sample_data(ps)$age_binned) <- list(Young100="(0,100]", Mid100to200="(100,200]", Old200="(200,400]")
sample_data(ps)$family_relationship=gsub(" ","",sample_data(ps)$family_relationship)
pslog <- transform_sample_counts(ps, function(x) log(1 + x))
out.wuf.log <- ordinate(pslog, method = "MDS", distance = "wunifrac")
evals <- out.wuf.log$values$Eigenvalues
plot_ordination(pslog, out.wuf.log, color = "age_binned") + labs(col = "Binned Age") +coord_fixed(sqrt(evals[2] / evals[1]))
  
rel_abund <- t(apply(otu_table(ps), 1, function(x) x / sum(x)))
qplot(rel_abund[, 12], geom = "histogram",binwidth=0.05) +
  xlab("Relative abundance")


```

```{r}
# Different Ordination Projections

outliers <- c("F5D165", "F6D165", "M3D175", "M4D175", "M5D175", "M6D175")
ps <- prune_samples(!(sample_names(ps) %in% outliers), ps)

which(!rowSums(otu_table(ps)) > 1000)

ps <- prune_samples(rowSums(otu_table(ps)) > 1000, ps)
pslog <- transform_sample_counts(ps, function(x) log(1 + x))

out.pcoa.log <- ordinate(pslog,  method = "MDS", distance = "bray")
evals <- out.pcoa.log$values[,1]
plot_ordination(pslog, out.pcoa.log, color = "age_binned",
                  shape = "family_relationship") +
  labs(col = "Binned Age", shape = "Litter")+
  coord_fixed(sqrt(evals[2] / evals[1]))

out.dpcoa.log <- ordinate(pslog, method = "DPCoA")
evals <- out.dpcoa.log$eig
plot_ordination(pslog, out.dpcoa.log, color = "age_binned", label= "SampleID",
                  shape = "family_relationship") +
  labs(col = "Binned Age", shape = "Litter")+
  coord_fixed(sqrt(evals[2] / evals[1]))


# Supervised learning
#--> faire jusque la (non compris) 
```


